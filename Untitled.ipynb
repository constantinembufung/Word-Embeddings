{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/online1/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import collections \n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "#download the data\n",
    "import bz2\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "#visualise the word\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import nltk # standard preprocessing\n",
    "import operator #sorting items in dictionary by value\n",
    "\n",
    "from math import ceil\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "This code downloads a [dataset](http://www.evanjones.ca/software/wikipedia2text.html) consisting of several Wikipedia articles totaling up to roughly 61 megabytes. Additionally the code makes sure the file has the correct size after downloading it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to download: wikipedia2text-extracted.txt.bz2\n",
      "\n",
      "Download complete!\n",
      "Found and verified wikipedia2text-extracted.txt.bz2\n"
     ]
    }
   ],
   "source": [
    "url = 'http://www.evanjones.ca/software/'\n",
    "\n",
    "def download_data(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present and make sure it's the right size\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        print('Attempting to download:', filename)\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "        print('\\nDownload complete!')\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?.'\n",
    "        )\n",
    "    return filename\n",
    "\n",
    "filename = download_data('wikipedia2text-extracted.txt.bz2', 18377035)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data without processing\n",
    "Reads data as it is to a string and tokenize it using spaces and returns a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 11631723\n",
      "Example words (start):  ['Propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
      "Example words (end):  ['useless', 'for', 'cultivation', '.', 'and', 'people', 'have', 'sex', 'there', '.']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "    \n",
    "    with bz2.BZ2File(filename) as f:\n",
    "        data = []\n",
    "        file_string = f.read().decode('utf-8')\n",
    "        file_string = nltk.word_tokenize(file_string)\n",
    "        data.extend(file_string)\n",
    "    return data\n",
    "\n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))\n",
    "print('Example words (start): ', words[:10])\n",
    "print('Example words (end): ', words[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data with Preprocessing with NLTK\n",
    "Reads data as it is to a string, convert to lower-case and tokenize it using the nltk library. This code reads data in 1MB portions as processing the full text at once slows down the task and returns a list of words. You will have to download the necessary tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "Data size 3360286\n",
      "Example words (start):  ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed', 'at', 'influencing']\n",
      "Example words (end):  ['favorable', 'long-term', 'outcomes', 'for', 'around', 'half', 'of', 'those', 'diagnosed', 'with']\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    \"\"\" Extract the first file enclosed in a zip file as a list of words\n",
    "    and pre-processes it using the nltk python library\n",
    "    \"\"\"\n",
    "    \n",
    "    with bz2.BZ2File(filename) as f:\n",
    "        data = []\n",
    "        file_size = os.stat(filename).st_size\n",
    "        chuck_size = 1024*1024 #reading 1MB at a time as the dataset is large\n",
    "        print('Reading data...')\n",
    "        for i in range(ceil(file_size//chuck_size) + 1):\n",
    "            bytes_to_read = min(chuck_size, file_size - (i * chuck_size))\n",
    "            file_string = f.read(bytes_to_read).decode('utf-8')\n",
    "            file_string = file_string.lower()\n",
    "            #tokenize a string to word residing in a list\n",
    "            file_string = nltk.word_tokenize(file_string)\n",
    "            data.extend(file_string)\n",
    "    \n",
    "    return data\n",
    "\n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))\n",
    "print('Example words (start): ', words[:10])\n",
    "print('Example words (end): ', words[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dictionaries\n",
    "Builds the following. To understand each of these elements, let us also assume the text \"I like to go to school\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 69215], ('the', 226881), (',', 184013), ('.', 120944), ('of', 116323), ('and', 88638), ('in', 77802), ('to', 65636), ('a', 58908), ('is', 30858)]\n",
      "Sample data [1721, 9, 8, 16471, 223, 4, 5165, 4456, 26, 11590]\n"
     ]
    }
   ],
   "source": [
    "# we restrict our vocabulary size to 50000\n",
    "vocabulary_size = 50000\n",
    "\n",
    "def build_dataset(words):\n",
    "    count = [['UNK', -1]]\n",
    "    # Gets only the vocabulary_size most common words as the vocabulary\n",
    "    # All the other words will be replaced with UNK token\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "    dictionary = dict()\n",
    "    \n",
    "    # Create an ID for each word by giving the current length of the dictionary\n",
    "    # And adding that item to the dictionary\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        \n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    # Traverse through all the text we have and produce a list\n",
    "    # where each element corresponds to the ID of the word found at that index\n",
    "    \n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0 #dictionary['UNK']\n",
    "            unk_count = unk_count + 1\n",
    "        data.append(index)\n",
    "        \n",
    "    #update the count variavl ewith the number of UNK occurance\n",
    "    count[0][1] = unk_count\n",
    "    \n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    \n",
    "    assert len(dictionary) == vocabulary_size\n",
    "    \n",
    "    return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Most common words (+UNK)', count[:10])\n",
    "print('Sample data', data[:10])\n",
    "del words  # Hint to reduce memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batches of Data for Skip-Gram\n",
    "Generates a batch or target words (batch) and a batch of corresponding context words (labels). It reads 2*window_size+1 words at a time (called a span) and create 2*window_size datapoints in a single span. The function continue in this manner until batch_size datapoints are created. Everytime we reach the end of the word sequence, we start from beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: ['propaganda', 'is', 'a', 'concerted', 'set', 'of', 'messages', 'aimed']\n",
      "\n",
      "with window_size = 1:\n",
      "    batch: ['is', 'is', 'a', 'a', 'concerted', 'concerted', 'set', 'set']\n",
      "    labels: ['propaganda', 'a', 'is', 'concerted', 'a', 'set', 'concerted', 'of']\n",
      "\n",
      "with window_size = 2:\n",
      "    batch: ['a', 'a', 'a', 'a', 'concerted', 'concerted', 'concerted', 'concerted']\n",
      "    labels: ['propaganda', 'is', 'concerted', 'set', 'is', 'a', 'set', 'of']\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "def generate_batch_skip_gram(batch_size, window_size):\n",
    "    #data_index is updated by 1 everywhere we read a data point\n",
    "    global data_index\n",
    "    \n",
    "    #two numpy arras to hold target words (batch)\n",
    "    # and context words (labels)\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    span = 2 * window_size + 1\n",
    "    # the buffer holds the data contained within the span\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    #fill the buffer and update the data_index\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "        \n",
    "    #this is the number of context words we sample for single target word\n",
    "    num_samples = 2 * window_size\n",
    "    \n",
    "    # We break the batch reading into two for loops\n",
    "    # The inner for loop fills in the batch and labels with \n",
    "    # num_samples data points using data contained withing the span\n",
    "    # The outper for loop repeat this for batch_size//num_samples times\n",
    "    # to produce a full batch\n",
    "    for i in range(batch_size // num_samples):\n",
    "        k = 0\n",
    "        #avoid the target word itself as a prediction \n",
    "        #fill in batch and label numpy arrays\n",
    "        for j in list(range(window_size))+list(range(window_size+1,2*window_size+1)):\n",
    "            batch[i * num_samples + k] = buffer[window_size]\n",
    "            labels[i * num_samples + k, 0] = buffer[j]\n",
    "            k += 1 \n",
    "        # Everytime we read num_samples data points,\n",
    "        # we have created the maximum number of datapoints possible\n",
    "        # withing a single span, so we need to move the span by 1\n",
    "        # to create a fresh new span\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    return batch, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "for window_size in [1, 2]:\n",
    "    data_index = 0\n",
    "    batch, labels = generate_batch_skip_gram(batch_size=8, window_size=window_size)\n",
    "    print('\\nwith window_size = %d:' %window_size)\n",
    "    print('    batch:', [reverse_dictionary[bi] for bi in batch])\n",
    "    print('    labels:', [reverse_dictionary[li] for li in labels.reshape(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Algorithm\n",
    "### Defining Hyperparameters\n",
    "Here we define several hyperparameters including batch_size (amount of samples in a single batch) embedding_size (size of embedding vectors) window_size (context window size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
